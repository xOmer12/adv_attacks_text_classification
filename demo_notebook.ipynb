{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/romh/miniconda3/envs/cluster/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_labels_df = pd.read_csv('data/test_labels.csv')\n",
    "\n",
    "train_df = train_df[['id', 'comment_text', 'toxic']]\n",
    "negative_sample_train = train_df[train_df['toxic'] == 0].sample(frac=0.1)\n",
    "positive_sample_train = train_df[train_df['toxic'] == 1]\n",
    "train_df = pd.concat([negative_sample_train, positive_sample_train])\n",
    "test_labels_df = test_labels_df[['id', 'toxic']]\n",
    "\n",
    "test_df = pd.merge(test_df, test_labels_df, on='id', how='inner')\n",
    "test_df = test_df[test_df['toxic'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data['comment_text'].iloc[idx]\n",
    "        label = self.data['toxic'].iloc[idx]\n",
    "        inputs = self.tokenizer(text, return_tensors='pt', padding='max_length', max_length=128, truncation=True)\n",
    "        inputs['label'] = torch.tensor(label)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, transformer_model, freeze_transformer=True, normalize_PP_T=True):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(transformer_model)\n",
    "        # Freeze the transformer model\n",
    "        if freeze_transformer:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.set_PP_T(normalize=normalize_PP_T)\n",
    "\n",
    "    def set_PP_T(self, normalize=True):\n",
    "        self.embeddings_matrix = self.get_static_embeddings_matrix()\n",
    "        self.PP_T = self.embeddings_matrix.T @ self.embeddings_matrix\n",
    "        if normalize:\n",
    "            # normalize using frobenius norm\n",
    "            self.PP_T = self.PP_T / torch.norm(self.PP_T, p='fro')\n",
    "\n",
    "    def get_static_embeddings_matrix(self):\n",
    "        return self.model.get_input_embeddings().weight\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, inputs_embeds=inputs_embeds)\n",
    "        cls_token = outputs.last_hidden_state[:, 0]\n",
    "        cls_token = self.fc(cls_token)\n",
    "        return self.sigmoid(cls_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TextClassifier('bert-base-uncased').to(device)\n",
    "\n",
    "train_dataset = TextDataset(train_df, model.tokenizer)\n",
    "test_dataset = TextDataset(test_df, model.tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:33<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.6905189267659592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:34<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.6834584090669277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:34<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.6777997683670561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:34<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.6711571216583252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [00:34<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.6653854665109666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        input_ids = input_ids.squeeze(1)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        attention_mask = attention_mask.view(input_ids.shape)\n",
    "        labels = batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}/{5}, Loss: {epoch_loss/len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:18<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7550720560192566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        input_ids = input_ids.squeeze(1)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        attention_mask = attention_mask.view(input_ids.shape)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predicted = torch.round(outputs.squeeze())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy: {correct/total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAdvDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data['comment_text'].iloc[idx]\n",
    "        label = self.data['toxic'].iloc[idx]\n",
    "        inputs = self.tokenizer(text, return_tensors='pt')\n",
    "        inputs['label'] = torch.tensor(label)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_test_dataset = TextAdvDataset(test_df, model.tokenizer)\n",
    "adv_test_loader = DataLoader(adv_test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvRunner:\n",
    "    def __init__(self, model, criterion, optimizer, device, alpha=1e-3, suffix_len=20, suffix_char='!'):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "        self.suffix_len = suffix_len\n",
    "\n",
    "        self.embeddings_matrix = self.model.embeddings_matrix\n",
    "        self.PP_T = self.model.PP_T.to(self.device)\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.model.eval()\n",
    "\n",
    "        self.suffix = suffix_char * suffix_len\n",
    "\n",
    "    # TODO: check if this is the correct projection (for now we ignore it)\n",
    "    def project(self, inputs_embeds):\n",
    "        inputs_embeds[:, -(self.suffix_len+1):-1] = torch.einsum(\n",
    "            'ij,bsj->bsi', self.PP_T, inputs_embeds[:, -(self.suffix_len+1):-1])\n",
    "        return inputs_embeds\n",
    "\n",
    "    def decode(self, input_ids, skip_special_tokens=True):\n",
    "        return self.tokenizer.decode(input_ids, skip_special_tokens=skip_special_tokens)\n",
    "    \n",
    "    def FGSM_step(self, inputs):\n",
    "        # Decode original input text and append the suffix\n",
    "        original_text = self.decode(inputs['input_ids'][0])\n",
    "        perturbed_text = original_text + self.suffix\n",
    "\n",
    "        # Tokenize perturbed text\n",
    "        tokenized = self.tokenizer(perturbed_text, return_tensors='pt').to(self.device)\n",
    "        input_ids = tokenized['input_ids']\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "        label = inputs['label'].float().to(self.device)\n",
    "\n",
    "        print(f'Starting Attack: {perturbed_text}')\n",
    "        print(f'Original Label: {label.item()}')\n",
    "\n",
    "        # Get original text loss\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        original_text_pred = outputs.item()\n",
    "        print(f'Original Prediction: {original_text_pred}')\n",
    "        text_loss = self.criterion(outputs, label.unsqueeze(0).unsqueeze(0))\n",
    "        print(f'Original text loss: {text_loss.item()}')\n",
    "\n",
    "        # Get embeddings and enable gradient tracking\n",
    "        inputs_embeds = self.embeddings_matrix[input_ids].clone().detach().to(self.device)\n",
    "        inputs_embeds.requires_grad_(True)\n",
    "\n",
    "        # Forward pass with embeddings\n",
    "        outputs = self.model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        original_emb_pred = outputs.item()\n",
    "        print(f'Original Embedding Prediction: {original_emb_pred}')\n",
    "        loss = self.criterion(outputs, label.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        print(f'Original embedding loss: {loss.item()}')\n",
    "\n",
    "        # Extract gradients corresponding to suffix tokens only\n",
    "        perturbation = inputs_embeds.grad[:, -(self.suffix_len):-1].sign()\n",
    "\n",
    "        # Apply perturbation\n",
    "        with torch.no_grad():\n",
    "            inputs_embeds[:, -(self.suffix_len):-1] += self.alpha * perturbation\n",
    "            # inputs_embeds = self.project(inputs_embeds)\n",
    "\n",
    "        # Map perturbed embeddings back to discrete tokens (nearest embeddings)\n",
    "        with torch.no_grad():\n",
    "            distances = torch.cdist(inputs_embeds, self.embeddings_matrix)\n",
    "            perturbed_input_ids = distances.argmin(dim=-1)\n",
    "\n",
    "        # Decode new perturbed text\n",
    "        new_text = self.decode(perturbed_input_ids[0])\n",
    "        print(f'Perturbed Text: {new_text}')\n",
    "\n",
    "        # Get perturbed text loss\n",
    "        perturbed_outputs = self.model(input_ids=perturbed_input_ids, attention_mask=attention_mask)\n",
    "        perturbed_text_pred = perturbed_outputs.item()\n",
    "        print(f'Perturbed Prediction: {perturbed_text_pred}')\n",
    "        perturbed_loss = self.criterion(perturbed_outputs, label.unsqueeze(0).unsqueeze(0))\n",
    "        print(f'Perturbed text loss: {perturbed_loss.item()}')\n",
    "\n",
    "        # Evaluate perturbed inputs_embeds on the model\n",
    "        perturbed_emb_outputs = self.model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        perturbed_emb_pred = perturbed_emb_outputs.item()\n",
    "        print(f'Perturbed Embedding Prediction: {perturbed_emb_pred}')\n",
    "        perturbed_emb_loss = nn.BCELoss()(perturbed_emb_outputs, label.unsqueeze(0).unsqueeze(0))\n",
    "        print(f'Perturbed embedding loss: {perturbed_emb_loss.item()}')\n",
    "\n",
    "        return perturbed_input_ids\n",
    "    \n",
    "    def PGD_step(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Attack on Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Attack: thank you for understanding. i think very highly of you and would not revert without discussion.!!!!!!!!!!!!!!!!!!!!\n",
      "Original Label: 0.0\n",
      "Original Prediction: 0.5034220814704895\n",
      "Original text loss: 0.7000148892402649\n",
      "Original Embedding Prediction: 0.5034220814704895\n",
      "Original embedding loss: 0.7000148892402649\n",
      "Perturbed Text: thank you for understanding. i think very highly of you and would not revert without discussion.! instead instead but some some but and and and and and. assert lay of donovan mace escaped an\n",
      "Perturbed Prediction: 0.5381074547767639\n",
      "Perturbed text loss: 0.7724230289459229\n",
      "Perturbed Embedding Prediction: 0.6985443830490112\n",
      "Perturbed embedding loss: 1.1991324424743652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  4067,  2017,  2005,  4824,  1012,  1045,  2228,  2200,  3811,\n",
       "          1997,  2017,  1998,  2052,  2025,  7065,  8743,  2302,  6594,  1012,\n",
       "           999,  2612,  2612,  2021,  2070,  2070,  2021,  1998,  1998,  1998,\n",
       "          1998,  1998,  1012, 20865,  3913,  1997, 12729, 19382,  6376,  2019,\n",
       "           102]], device='cuda:0')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take example from test dataset\n",
    "inputs = next(iter(adv_test_loader))\n",
    "advrunner = AdvRunner(model, criterion, optimizer, device, alpha=1)\n",
    "\n",
    "single_input = {key: inputs[key][0] for key in inputs.keys()}\n",
    "advrunner.FGSM_step(single_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
